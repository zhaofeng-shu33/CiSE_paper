\documentclass[notheorems]{beamer}
\usetheme{Lab2C}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[square,sort,comma,numbers]{natbib}
\input{macros.tex}
\newif\ifbeamer
\beamertrue

\title{Reproducing Scientific Experiment with Cloud DevOps}
\author{Feng Zhao}
%\institute{\inst{1}Dept. of Electronic Engineering, Tsinghua University
%\and \inst{2}Tsinghua-Berkeley Shenzhen Institute, Tsinghua University}
\date{\today}
\begin{document}
\begin{frame}
	\titlepage
\end{frame}
\section*{Outline}
\begin{frame}
	\tableofcontents
\end{frame}

\section{Introduction}
\subsection{Outlier Detection and Existing Method}
\begin{frame}
\frametitle{Outlier Detection}
	\begin{columns}
		\column{5cm}
		\begin{figure}
			\includegraphics[width=4cm]{pic/cloud_computing.jpeg}
		\end{figure}
		\column{5cm}
		Outlier detection is the identification of abnormal data which differs from the majority of the data.
	\end{columns}
\begin{figure}
	\centering
	\begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{pic/github_actions.jpg}
		\caption{DevOps workflow}
	\end{subfigure}~~~~~
	\begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{pic/general_workflow.png}
		\caption{Cloud DevOps provider}
	\end{subfigure}
\end{figure}  
\end{frame}
\begin{frame}
\frametitle{Existing Method}
	\begin{columns}
\column{0.4\textwidth}
\column{0.6\textwidth}
\quad Shortcomings:
\begin{itemize}
\item provide the number of outliers
\item hyperparameters matter
\end{itemize}
\end{columns}
\end{frame}
\subsection{Related Work}
\begin{frame}
	\frametitle{Related Work}
\begin{itemize}
\item graph based clustering method
	\begin{enumerate}
		\item \citeauthor{de} in \citeyear{de} proposed a hierarchical density estimate method 
%to do outlier detection
		\item \citeauthor{mac} in \citeyear{mac} proposed a minimum average cost clustering method 
%which shares the same hierarchical clustering structure with info-clustering.
		\item \citeauthor{ic} in \citeyear{ic} proposed info-clustering method
%which is a hierarchical clustering method.
	\end{enumerate}
\item Graph partition algorithm
\begin{enumerate}
\item \citeauthor{psp} in \citeyear{psp} proposed an algorithm to compute the principal sequence of partition 
\item \citeauthor{pmf} in \citeyear{pmf} proposed a parametric algorithm for the same task 
\end{enumerate}
\end{itemize}
Our work
\begin{itemize}
\item Info-Detection method
\item Improved algorithm for principal sequence of partition
\end{itemize}
\end{frame}
\section{Formulation of Info-Detection}
\subsection{Info-Clustering}
\begin{frame}
\frametitle{Review of Info-Clustering}
\begin{enumerate}
\item directed graph $G(V, E)$; Node set $Z_V=\{Z_i | i \in V\}$
\item partition $\P=\{C_1, \dots, C_k\}, \bigcup_{i=1}^k C_i = V$ and $i\neq j \Rightarrow C_i \cap C_j = \emptyset$
\item $\Pi$ is the collection of all partitions of $V$, $\Pi' = \Pi \backslash \{V\}$
\item $f(\cdot)$ is the graph in-cut function, $f(C)=\sum_{i \neq C, j\in C, (i,j) \in E} w_{ij}$
\item $f[\cdot]$ is a function defined on $\Pi$ by $f[\P]=\sum_{C\in \P}f(C)$
\end{enumerate}
\begin{definition}[info-cluster]
Given $G(V, E)$, the cluster set $C_{\gamma}(Z_V)$ is defined as 
\begin{align}
I_{\mathcal{P}}(Z_V) & = \frac{f[\P]}{ \abs{\mathcal{P}} - 1}\\
I(Z_V) & = \min_{\mathcal{P} \in \Pi'(V)} I_{\mathcal{P}}(Z_V) \\
C_{\gamma}(Z_V) & := \textrm{maximal}\{ B \in V \vert \, \abs{B} > 1, I(Z_B) > \gamma \}
\end{align}
\end{definition}
\end{frame}
\begin{frame}{Review of Info-Clustering}
\begin{enumerate}
\item  every two sets $C(Z_V)=\bigcup_{\gamma \geq 0} C_{\gamma}(Z_V)$ are either disjoint or have subset relationship
\item sets from $C(Z_V)$ can be arranged in a clustering tree $\mathcal{T}$; The leaf node of $\mathcal{T}$ is $\{j\}$.
\item the largest threshold value $\gamma_N = \max_{A\subseteq V, \abs{A}>1} I(Z_A)$; The maximal set to achieve this is denoted $B$; $\gamma_N = I(Z_B)$.
\end{enumerate}
Example
\begin{columns}
\column{0.45\textwidth}
\column{0.55\textwidth}
	\begin{equation*}
	C_{\gamma}(Z_V)	=\begin{cases}
					\{\{1,2,3\}\} & \gamma < 2 \\
					\{\{1,3\}\} & 2\leq \gamma < 5 \\
					\emptyset & \gamma \geq \gamma_N = 5
		\end{cases}
	\end{equation*}
\end{columns}
\end{frame}
\subsection{Info-Detection}
\begin{frame}
	\frametitle{Info-Detection Method}
\begin{itemize}
\item For $\gamma_N = I(Z_B)$, nodes in $B$ are inliers and nodes in $V\backslash B$ are outliers.
\item The ourlier score $Z_j$ for $j \in V\backslash B$ is measured with the depth of the set $\{j\}$ in the hierarchical tree.
\end{itemize}
Example
\end{frame}
\begin{frame}{Prediction Scheme for Info-Detection}
For newly added node $i'$. Let $V'=B\cup \{i'\}$, we can compute $\gamma'_N$ for $G(V', E(V'))$.
$ \gamma'_N > \gamma_N \iff i'$ is normal observation. We found that
\begin{proposition}
\begin{equation}
\gamma'_N > \gamma_N \iff  \sum_{i \in B} w_{ii'} > \gamma_N 
\end{equation}
\end{proposition}
\end{frame}

\section{Algorithms}
\frame{\tableofcontents[currentsection]}
\begin{frame}{Review of Principal Sequence of Partition}
It has been found the mathematical structure of info-clustering is the same with that of principal sequence of partition.
\begin{definition}[Principal Sequence of Partition]
\begin{align}
h_{\P}(\lambda) &=  f[\P] - \abs{\P} \lambda  \label{eq:hPL}\\
h(\lambda) &= \min_{\P \in \Pi'(V)} h_{\P}(\lambda) \label{eq:hLambda}
\end{align}
The optimal partitions $\P_1, \dots, \P_k$ for $h(\lambda)$ are nested such that $\P_1 \succeq \P_2 \dots \succeq \P_k$,  which are called principal sequence of partition (PSP) of the graph $G$.
\end{definition}
We call $\lambda^*$ a critical value for PSP if $\P_i, \P_{i+1}$ are both minimizer for $h(\lambda^*)$ in \eqref{eq:hLambda}.
The largest critical value is equal to $\gamma_N$ and the largest set in $\P_{k-1}$ is equal to $B$.
\end{frame}
\begin{frame}
\frametitle{Review of Narayanan's Algorithm to compute PSP}
\begin{columns}
\column{0.5\textwidth}
\algsetup{linenosize=\tiny}
\begin{algorithm}[H]
\caption*{Narayanan's Algorithm}\label{alg:psp}
{\tiny
\begin{algorithmic}[1]
\REQUIRE a directed graph $G(V,E)$
\ENSURE A sorted critical value array \textbf{L} and a reverse ordered array \textbf{PSP} containing $\P_1,\dots, \P_k$.
\STATE \textbf{L}  $\leftarrow$ empty list.
\STATE $Q\leftarrow \{V\}, P \leftarrow \{ \{i \} | i \in V\}$
\STATE $\mathbf{PSP}= [Q, P]$
\STATE \texttt{Split}$(Q,P)$
\STATE sort $L$ and sort $\mathbf{PSP}$ with respect to $\succeq$ 
\FUNCTION{\texttt{Split}$(Q,P)$}
 \STATE\label{alg:gamma} $\gamma' = {1 \over \abs{P} - \abs{Q}} (f(P)-f(Q))$
 \STATE $h' = {1 \over \abs{P} - \abs{Q}}(\abs{P} f(Q) - \abs{Q} f(P))$
 \STATE $(\tilde{h}, P') = \texttt{DT}(f,\gamma')$
 \IF{$\tilde{h} = h'$}
 	\STATE insert $\gamma'$ to $\mathbf{L}$
 \ELSE
 	\STATE insert $P'$ to $\mathbf{PSP}$
 	\STATE \texttt{Split}$(Q, P')$
 	\STATE \texttt{Split}$(P',P)$
 \ENDIF
\ENDFUNCTION
\end{algorithmic}
}
\end{algorithm}
The whole graph $G$ is used in each computation of \texttt{DT} routine.
\column{0.5\textwidth}
Example
\begin{align*}
\P_0  & = \{\{1,2,3\}\} \\
\P_1  & = \{\{1,3\},\{2\}\} \\
\P_2  & = \{\{1\},\{2\},\{3\}\} 
\end{align*}
\end{columns}
\end{frame}
\begin{frame}
\frametitle{Improved PSP Algorithm}
\begin{columns}
\column{0.5\textwidth}
\vskip -1.2em
\algsetup{linenosize=\tiny}
\begin{algorithm}[H]
\caption*{{\footnotesize Improved PSP Algorithm}}
{\tiny
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$
		\ENSURE a hierarchical tree $\mathcal{T}(K, E)$ where $K \subseteq 2^{V}$ is node set and $E$ is edge set.
		\STATE initialize tree $\mathcal{T}$ with $V$ as root node, $\{j\}(j\leq \abs{V})$ as leaf node and no stem node.
		\STATE \texttt{Split}($G, V$)
		\FUNCTION{\texttt{Split}($\widetilde{G}, \widetilde{V}$)}
		\STATE $w$ is the summation of all edge weights of $\widetilde{G}$ 
		\STATE $\gamma' = \frac{w}{\abs{V(\widetilde{G})}-1}$ where $V(\widetilde{G})$ is node set of graph $\widetilde{G}$ \label{alg:gamma_apostrophe}
		\STATE $(\tilde{h}, P') = \texttt{DT}(\widetilde{G}, \gamma')$ 
		\IF{$\tilde{h} = - \gamma'$}
		\STATE add edge weight $\gamma'$ in $\mathcal{T}$ from $\widetilde{V}$ to its children
		\ELSE
		\FOR{$S$ in $P'$ and $\abs{S}>1$}
		\STATE make children of $\widetilde{V}$ in $\mathcal{T}$ have new parent $S$		
		\STATE make the parent of $S$ be $\widetilde{V}$
		\STATE \texttt{Split}($\widetilde{G}[S], S$) where $\widetilde{G}[S]$ is the subgraph of $\widetilde{G}$ restricted on $S$
		\STATE contract $S$ to a single node in $\widetilde{G}$ % graph \widetilde{G} is modified
		\ENDFOR 
		\STATE \texttt{Split}($\widetilde{G}, \widetilde{V}$)		
		\ENDIF
		\ENDFUNCTION
	\end{algorithmic}
}
\end{algorithm}
\vskip -1.2em
Smaller graph $G$ is used in each computation of \texttt{DT} routine.
\column{0.5\textwidth}
Example
Time complexity for dense graph
\begin{itemize}
\item Narayanan's: $O(n^5)$
\item Ours: $O(n^4)$
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Empirical Comparison for three implementation of PSP}
\end{frame}
\section{Experimental Results}
\begin{frame}
	\frametitle{Experimental Results}
\begin{table}
\centering
\begin{tabular}{ccccc}
\hline
              &  GaussianBlob   &      Moon       &  Lymphography  &     Glass     \\
\hline
   Type    & artificial & artificial & real-world & real-world \\
   N(Inliers) & 255  & 300 & 142  & 207 \\
   N(outliers)   & 45 &  45  & 6 & 7 \\
   N(features)   & 2 &  2  & 19 & 7  \\
\hline
\end{tabular}
\end{table}
\begin{itemize}
\item True Positive Rate (TPR)  $=\frac{N(\textrm{detected inliers})}{N(\textrm{inliners})} \geq 90\% $
\item True Negative Rate (TNR) $=\frac{N(\textrm{detected outliers})}{N(\textrm{outliers})}$
\end{itemize}

\end{frame}	
\section{Conclusion}
\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item Formulate Info-Detection method
\item Propose improved principal sequence of partition algorithm 
\item Demonstrate Info-Detection performs well on some datasets
\end{itemize}
\end{frame}
%\section*{Reference}
%\begin{frame}
%\frametitle{Reference}
%\setbeamertemplate{bibliography item}[triangle]
%\bibliographystyle{plainnat}
%{\footnotesize
%\bibliography{id}
%}
%\end{frame}
\begin{frame}
\frametitle{}
\begin{block}{}
\centering
Thank you!
\end{block}
\end{frame}
\end{document}
